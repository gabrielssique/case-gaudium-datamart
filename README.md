# ğŸš€ Desafio TÃ©cnico - Engenheiro de Dados JÃºnior (Gaudium)
# Autor: Gabriel da Silva Siqueira

## ğŸ§¾ DescriÃ§Ã£o
Este projeto tem como objetivo aplicar tÃ©cnicas de modelagem dimensional a partir de uma tabela Ãºnica de dados brutos contendo informaÃ§Ãµes de vendas, produtos e clientes.  
A partir dessa base, foram criadas tabelas fato e dimensÃµes utilizando PySpark e Spark SQL.

## ğŸ“ Estrutura dos Dados

Arquivo de entrada: `dados_brutos.csv`  
Colunas disponÃ­veis:
- nome_cliente
- cidade
- estado
- nome_produto
- categoria
- fabricante
- data
- qtd_vendida
- valor_total

## ğŸ— Modelagem Criada

- **dim_cliente**  
  ContÃ©m informaÃ§Ãµes Ãºnicas sobre os clientes.

- **dim_produto**  
  ContÃ©m informaÃ§Ãµes Ãºnicas sobre os produtos vendidos.

- **dim_localidade**  
  ContÃ©m informaÃ§Ãµes Ãºnicas de cidade e estado.

- **dim_tempo**  
  Derivada da coluna `data`, com ano, mÃªs e dia.

- **fato_vendas**  
  Tabela principal contendo os fatos de vendas (com chaves para as dimensÃµes).

## ğŸ›  Tecnologias Utilizadas

- PySpark
- Spark SQL
- Git e GitHub

## ğŸ“‚ Arquivos no RepositÃ³rio

- `casegaudium.ipynb` - Notebook com todo o cÃ³digo usado.
- `dim_cliente.csv`, `dim_produto.csv`, `dim_localidade.csv`, `dim_tempo.csv`, `fato_vendas.csv` - Tabelas exportadas.
- `README.md` - Este documento.

## â–¶ï¸ Como Executar

1. Certifique-se de ter um ambiente com suporte a PySpark (ex: Databricks, AWS EMR, mÃ¡quina local com Spark instalado).
2. Suba o arquivo `dados_brutos.csv` e o notebook.
3. Execute o notebook `casegaudium.ipynb`.
4. As tabelas modeladas serÃ£o salvas na pasta `tabelas_fato_dim` em formato `.csv`.

## âœ… ObservaÃ§Ãµes

- A modelagem foi desenvolvida com base no esquema estrela.
- O foco principal foi demonstrar a criaÃ§Ã£o e transformaÃ§Ã£o dos dados de forma limpa, escalÃ¡vel e eficiente.